{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "980160ab-98cf-4eb4-a87a-f21bbff36545",
   "metadata": {},
   "source": [
    "# 1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fdda7a-eded-443f-afb5-8328506aa59f",
   "metadata": {},
   "source": [
    "    This project was inspired by the 2021 paper, \"Searching for young stellar objects through SEDs by machine learning\", by Chiu et. al. In it, the researchers train a machine learning model (a neural network) on the Spitzer From Molecular Cores to Planet-Forming Disks (C2D) dataset. They attempted to successfully classify 3 general types of object; stars, young stellar objects (YSOs), and galaxies. This machine learning classification will aid the identification of YSOs, as a major issue in the contemporary field is that YSO classification is somewhat demanding in both manual effort, as well as quite uncertain due to the connection between YSOs and stellar sources. If we are able to identify where, in the parameter space of common measured flux bands, YSOs are typically located; we can be more confident and efficient in classification of other sources with potential to be YSOs.\n",
    "    We must note that the spectral features of YSOs are what enables this. As the delivery document from the C2D dataset (Evans et. al. 2014) states, the main quality of a YSO is the presence of a significant dust component, which appears as an excess in infrared wavelengths. Comparably, stars can be modelled by a stellar photosphere, which generally acts as a black-body, albeit with some reddening from ISM extinction. What this means is that, if we are to take data ranging from the near to far-IR, just as the Spitzer C2D dataset provides, we can make conclusions on an object's type based upon the ratio of flux between shorter and longer wavelengths. This is the basis of manual classification as was used in the C2D dataset. But of course, with machine learning, we need not manually evaluate this; we can treat every flux band as a 'dimension', and identify the 'region' in which YSOs are congregated; allowing for greater applicability to different datasets.\n",
    "    The Spitzer From Molecular Cores to Planet-Forming Disks (C2D) dataset used Spitzer to target both a variety of known YSOs, as well as all sources in five of the nearest large molecular clouds. The intention was to form a dataset which represents not only our best known examples of YSOs, but also which describes the environments in which these YSOs exist. The most important features of the dataset were the four IRAC bands and MIPS 1 band, corresponding to fluxes at 3.6, 4.5, 5.8, 8.0, and 24 microns, as well as the object_type, which was the manual label of each object as described in the delivery document. Note that the correct subset of the dataset is the '07 Full CLOUDS catalog, with 4.2 million objects, and contains all objects registered in those aforementioned molecular clouds. \n",
    "    In terms of data exploration, several steps were necessary before any machine learning took place. Firstly, an SQL query was given when retrieving the data, specifying that only sources without NULL in the 5 bands and object_type feature, were allowed to be given. This reduced the dataset immediately from 4.2 million to approximately 2.2 million objects. Secondly, upon reading the data in, I noticed that many objects that were not literally labelled as NULL, had flux values that ranged from 0 to negative values. Zero values are not ideal; I trimmed those from the dataset, as I desired only datapoints which at least some quantifiable flux. The negative flux values were, and are, extremely puzzling to me. Keep in mind some of these negative values were as large as -2 mJy. Given that \"negative\" flux is unphysical, I can only assume that it was the result of some form of bias-correction or photometric correction. However, no part of the delivery document indicates that negative values could arise. But, finding the sources of these data is not my prerogative; thus, I also cut these out, resulting in a final count of ~598,000 objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f962b88-76f7-41a8-bb60-ac5f94b1b364",
   "metadata": {},
   "source": [
    "# 2 - Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3c0a4-7556-40a8-9fce-83f0fe59e1aa",
   "metadata": {},
   "source": [
    "    My chosen algorithm will be Support Vector Machines (SVM). This is due to two simple aspects; firstly, as we've demonstrated before, the classes should spatially separate when put into the 5-dimensional \"flux\" parameter space, which will hopefully allow for hyperplanes or other kernel shapes to \"cut up\" our classes. Secondly, it is one of the more simple and reliable machine learning algorithms, and I typically value these over others. SVMs can especially be contrasted to the methods of the inspiratory paper, Chiu et. al., and their use of a somewhat opaque neural network. \n",
    "    As stated in the introduction, our main features will be the IRAC 1-4 bands, and MIPS 1 band, while our labels are described by the object_type. Note that object_type is in itself a complicated part of the dataset. It is not simply \"star, galaxy, YSO\", but instead defined by 33 detailed subtypes. More details can be found in the relevant section of the 2014 delivery document, but as a brief overview, the subtypes can generally be separated into four larger classifications. Subtype strings begin with their \"root\" classification, i.e. \"star\", \"YSOc\", and \"Galc\", thus we can easily group these together into three main classifications. All other labels can be generally grouped together into a fourth class, which we immediately discard. This includes several subtypes explicitly used for inconclusive classifications, objects with missing data, or simply unknown objects. Thus, after this processing, we return to a simple three-class system. This gives us a final count of ~454k \"discards\", ~140k stars, ~2,900 galaxies, and only ~1,000 YSOs.\n",
    "    When it comes to our model parameters, I have decided to forego hyperparameter optimization due to its inevitable computational complexity, and my desire for a certain level of simplicity in this project, as well as my own biases. If necessary, future implementations could explore different hyperparameters, but as we shall see later, my current level of success gives me no desire to do so. Thus, when using scikit's SVC Support Vector Machine implementation, I select the default C parameter of regularization (1.0), as well as the default Radial Basis Function (RBF) kernel. Qualitatively, this kernel is well-suited for the dataset; this is because two of our classes, YSOs and galaxies, are somewhat blobby (and thus a linear kernel would struggle).\n",
    "    In terms of assessment metrics, I found that traditional training/testing metrics are essentially useless for such an unbalanced dataset, as approximately 97% of our dataset consists of stars. So, as a thought experiment, we could mindlessly classify everything as a stellar source and still find \"amazing\" training and testing statistics. A more useful metric is a normalized confusion matrix, in which we normalize over the \"true\" labelling. This allows us to immediately see what proportion of the smaller classes is correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc119b0-2c26-4704-913c-28ca4ac125fb",
   "metadata": {},
   "source": [
    "# 3 - Initial Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f007fa2-3211-48b2-9c58-91764112dcb0",
   "metadata": {},
   "source": [
    "    Firstly, there is one, somewhat concerning aspect. As has been mentioned before, this project is inspired by, and intends to reproduce, some of the aspects of the 2021 Chiu et. al. paper. When initially attempting to reproduce figures from their work, I noted significant discrepancies between their work and my own, even though we source our data from the same set. Compare the two following figures; Chiu's Fig. 4(a), which is a log-log-log plot of the IRAC 3, IRAC 4, and MIPS 1 bands, and my own attempt to reproduce the figure with the same bounds on X, Y, and Z."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50258427-dc7a-4160-bea5-c8f6d1f9a65a",
   "metadata": {},
   "source": [
    "![chiu4a](chiu4a.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386e0d52-ada4-4e8e-83a0-4d9cf9c3e151",
   "metadata": {},
   "source": [
    "![4acomp](4acomp.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cbb06d-3ef3-4766-9b4d-3f69855239fb",
   "metadata": {},
   "source": [
    "    Note that while the YSO and galaxy populations are seemingly identical, my population of stars are much more numerous and extend to much smaller fluxes than compared to Fig. 4(a), which seemingly truncates the majority of the stars past a certain flux. As of now, I have no answer for this discrepancy; unfortunately, I have not had time to contact the researchers. I have reviewed the paper extensively and, as a layman, cannot identify any obvious clarifications of what may have caused this. Chiu et. al.'s data preparation is essentially similar to mine; removal of non-detections as the key point. The only culprit I can identify is what the researchers describe as removal of bias due to non-detections. Per my understanding, they noted that extremely small flux values are more likely complete non-detections with a small degree of photon flux/thermal noise being misidentified as meaningful information. Thus they replace these values with a hundredth of the \"reliable\" flux of that band. However, I do not think this is the cause; firstly, they are altering data, not removing it; so this does not explain the numerical discrepancy between my count (~140k) and their count (~59k) of identified stars. Secondly, they explicitly highlight that this change is small and has no effect on testing or training results. \n",
    "    Other possible answers include adjustments for extinction, or perhaps a simple step so elementary that it was deemed unnecessary to elaborate upon, and yet unknown to a layman such as I. It could also be that the researchers had similar problems with an unbalanced dataset, and thus removed a large proportion of stellar objects; but this would need to be done randomly/equally, to not introduce bias. I also considered that this was an artifact of stricter object_type grouping; I ran a modified version of my code which discarded everything not explicitly labeled as the three main classes (i.e. discarded all subtypes) and could not reproduce the discrepancy. I would most likely conclude that the researchers simply 'cut out' stars based on if they undershot a given flux value in one or more bands. This could be a viable selection method, but if so, it is necessary to explain it. I am concerned by the fact that I see no clear explanation. \n",
    "    Below, I also compare the researchers' corner plot, with accompanying histograms, to my own normalized histogram of those same three bands, for a more quantitative comparison at the differences between our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f032992-9e37-40c8-95fa-c3a196f0aedd",
   "metadata": {},
   "source": [
    "![chiucorner](chiucorner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27615fdc-ae24-4877-acc7-b8c3e85df019",
   "metadata": {},
   "source": [
    "![hists](hists.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98d858-7d51-4c0a-9669-a76836dc9868",
   "metadata": {},
   "source": [
    "    While it's impossible to say for certain, because the original plot is also normalized, this comparison makes it seem as if it is not an explicit 'cutting out' of stellar sources, but also a shift; as the researchers' version also preserves an overall Gaussian-ish natural shape, rather than a cliff that would arise from a 'cut'. Perhaps both a removal of points, and a shift in the form of the aforementioned 'non-detection bias correction', could be at play. Regardless, this issue will have no impact on the continuation of the project.\n",
    "    As part of the initial investigation, I applied an extremely simple linear SVM using scikit's LinearSVC. This allowed for a simple visual comparison between the original classification and a basic classification using a linear kernel. This was more of a trial run to see if the methods and code actually functioned, which they did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f70e2-6127-41d4-8f04-44f16e976590",
   "metadata": {},
   "source": [
    "![basicORIG](basicORIG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20063566-993b-48da-a5b3-0fe4e29b57cc",
   "metadata": {},
   "source": [
    "![basicLINSVM](basicLINSVM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796b3af4-e189-4bbb-ba64-b8bb8039ec18",
   "metadata": {},
   "source": [
    "    This test gave results as expected; the SVM generally struggles where the classes are intermixed finely, or in the cases of outliers, but correctly identified the general regions.\n",
    "    As a more advanced initial trial, I applied SVm with an RBF kernel and straitified 5-fold, in order to evaluate the training and testing scores. These scored were both in excess of 99.5%, again due to the aforementioned fact that 97% of our dataset is of a single class. A confusion matrix was a more appropriate form of evaluation, and thus I present one with normalization relative to the total number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b07b68-83c3-477d-8b69-a5b4303d0e3d",
   "metadata": {},
   "source": [
    "![confmatRBFnormall](confmatRBFnormall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae573e87-fe86-4273-862c-ff088a2c4270",
   "metadata": {},
   "source": [
    "    As we can see, the domination of the stellar classification makes this form of confusion matrix redundant. What is more useful, is normalizing over the \"true\" label. This allows us to see the fraction of each class that has been classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe36f59-ec0c-4ed3-80df-a4141890401f",
   "metadata": {},
   "source": [
    "![confmatRBFnormtrue](confmatRBFnormtrue.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0448e7c2-d99b-47cd-90b9-15f4886cac0e",
   "metadata": {},
   "source": [
    "    Here, we can see that galaxies and YSOs have success rates of around 90%, which is fairly high considering the dominance of the stellar class and the potential for misclassification in the \"mixed\" regions of both stars and other objects. Overall, however, there are two main steps that need to be taken. Firstly, we should attempt dimensionality reduction through PCA; as one can see in both of the above scatter and corner plots, certain variables (such as IRAC 3 and IRAC 4) have a fairly linear relation, and thus have a high potential for useful dimensional reduction. And if we can apply dimensionality reduction, this would allow our other algorithm, SVM, to be more efficient due to the decrease in complexity. Secondly, we should attempt some form of weighing when it comes to the importance of each class. We want to correctly classify as many YSOs as possible, but misclassification of stars is not a major concern; thus we could apply an inverse weight which prioritizes the smaller classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d09b0e-7c0c-4d31-af0c-563b440784a9",
   "metadata": {},
   "source": [
    "# 4 - Final Results and Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f1d7d-db98-4793-ab69-49af73d864fd",
   "metadata": {},
   "source": [
    "    The main form of optimization I focused on is dimensionality reduction through PCA, for the aforementioned reasons. Firstly, we can look at the dimensionality reduction applied to the original classification. Note that I leave the axes unlabelled as they are now somewhat disconnected principal components, not literal measurements. The same overall shape we could infer from the IRAC3-IRAC4-MIPS1 scatter plot is still present, despite the fact that we've reduced from 5 dimensions, which includes the IRAC 2 and 3 bands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e32204-663c-43a7-a9ff-096cead0ad07",
   "metadata": {},
   "source": [
    "![PCAorig](PCAorig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db887bab-dae0-4f9b-8bfe-23827d50dd11",
   "metadata": {},
   "source": [
    "    And here we have a simple RBF kernel applied to the reduced data. Note that, as mentioned before, the sheer number of stellar points tends to overpower the other two classifications in the 'border' zones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f7d05-163e-49db-b730-8b8cdcbdf0cc",
   "metadata": {},
   "source": [
    "![PCArbf](PCArbf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8c48f8-3774-45fb-9e17-a8e0f33c8d89",
   "metadata": {},
   "source": [
    "    This confusion matrix quantitatively supports this analysis; note how the rate of true classifications for YSOs and galaxies has dropped into the mid-80%s, while the number of misclassifications labelling them as stars is now significant. Obviously, this is a major indication that weighing is needed, in order to prioritize the classification of galaxies and YSOs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7410a5-ff8c-4984-b575-d59d2fab04dc",
   "metadata": {},
   "source": [
    "![PCArbfconf](PCArbfconf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ea78f-07ac-4ad2-813e-36882559c690",
   "metadata": {},
   "source": [
    "    Therefore, we can redo our RBF kernel SVM, applied to the full 5-dimensional dataset, but with a \"balanced\" class weighing which automatically adjust weights inversely proportional to class frequencies in the input data. I present a similar 3D scatter plot, but this figure is not illuminating on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba0df66-e748-4530-b49b-e6ae5ac929e7",
   "metadata": {},
   "source": [
    "![weightedRBF](weightedRBF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26052f37-8741-43be-a0ac-f9313a30b8c3",
   "metadata": {},
   "source": [
    "    What is more interesting is the corresponding confusion matrix, which shows that correct classifications have reached extremely high values, in excess of 98%, for all three classes. Essentially, when we sacrifice the few stars in the border regions by decreasing the importance of stellar weighing, all classifications increase simply because the stellar class can 'afford' to be misclassified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4bf2ce-ad27-452c-97dc-7bf915d2b1ff",
   "metadata": {},
   "source": [
    "![weightedRBFconf](weightedRBFconf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f94dffc-4599-4889-b575-8158cd736f46",
   "metadata": {},
   "source": [
    "    We can also apply this weighing to the reduced data, and visually see how the \"strengths\" of the galaxy and YSO classification push the borders in this two-dimensional presentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26df2fd-28a8-415d-86bd-923cf0120059",
   "metadata": {},
   "source": [
    "![weightedPCArbf](weightedPCArbf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f9ec9-fb00-4a19-83bf-034b2a656150",
   "metadata": {},
   "source": [
    "    And again, the confusion matrix itself is more illuminating. All three \"true-true\" classifications are at 96% or higher success, and now a degree of mutual competiton has actually arose between galaxy and YSO classification. But again, PCA is essentially a compromise; our full 5-dimensional SVM treatment still has a greater performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff3b8c-a107-44fe-a541-040c8c2925bb",
   "metadata": {},
   "source": [
    "![weightedPCArbfconf](weightedPCArbfconf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2ee1bf-a110-4584-9604-1f4d19ec5402",
   "metadata": {},
   "source": [
    "    For this particular dataset and problem, I believe that SVM is one of the best choices for classification. At the same time, I do note that the circumstances of the data tell us that any classification algorithm that can either be applied to the PCA'd data, or work in multiple dimensions (such as kNN), could achieve similar results. After all, in the end, this is a very simplistic case of classes which dominate well-defined regions; the only major complication is the imbalanced dataset, which we have mitigated through class weighing which is common among similar algorithms. Thus, while SVM performed well, a variety of other approaches could be equally successful.\n",
    "    Unfortunately, one aspect of this project I did not reach was the potential to apply these methods on the Spitzer Enhanced Imaging Products (SEIP) dataset. This would have allowed us to determine whether the algorithm, trained on a small subset of data in the form of the C2D set, which focuses on molecular clouds, could also function on the dataset containing the majority of Spitzer products, and thus acting as a much more general sample of the universe. I am simply limited by time and the space on my hard drive; I would emphasize, however, that it would be a simple matter of importing the dataset and selecting out the 5 bands before proceeding as normal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8725d08-9e5e-4f05-817b-4a56e71ee769",
   "metadata": {},
   "source": [
    "# 5 - Link to Github repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27864ad0-2df9-4673-8bfb-cf07d9ce07e5",
   "metadata": {},
   "source": [
    "GITHUB: https://github.com/AuthmaStasley/A416FinalProject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c09bd0-3ba9-4dc1-8639-555f143a1e0a",
   "metadata": {},
   "source": [
    "# 6 - AI Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d90dc1-3c58-46b8-b4a3-bba0161a6cb0",
   "metadata": {},
   "source": [
    "    No AI was used at any stage in this project; all code that did not organically spring from my mind or from following our in-class notebooks, or scikit documentation, was written using the assistance of ancient posts on StackOverflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb04f1f-1f07-4940-a95f-ebbdbec96cda",
   "metadata": {},
   "source": [
    "# 7 - Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc15a5-3be7-403e-b93d-04e5e3a392e3",
   "metadata": {},
   "source": [
    "    Link to Spitzer C2D dataset: https://irsa.ipac.caltech.edu/data/SPITZER/C2D/overview.html\n",
    "\n",
    "    \"Searching for young stellar objects through SEDs by machine learning\" by Chiu et. al., 2021, can be found here: https://ui.adsabs.harvard.edu/abs/2021A%26C....3600470C/abstract\n",
    "\n",
    "    \"Final delivery of data from the C2D Legacy Project\" by Evans et. al., 2014, can be found here: https://irsa.ipac.caltech.edu/data/SPITZER/C2D/doc/c2d_del_document.pdf (referenced as \"the delivery document\" in this report)\n",
    "\n",
    "    \"From Molecular Cores to Planet-forming Disks: An SIRTF Legacy Program\" by Evans et. al., 2003, can be found here: https://ui.adsabs.harvard.edu/abs/2003PASP..115..965E/abstract\n",
    "\n",
    "    \"The Spitzer c2d Legacy Results: Star-Formation Rates and Efficiencies; Evolution and Lifetimes\" by Evans et. al., 2009, can be found here: https://ui.adsabs.harvard.edu/abs/2009ApJS..181..321E/abstract"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
